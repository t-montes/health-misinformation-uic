{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.42s/it]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any\n",
    "\n",
    "base = '../data/Monant/data/{}.csv'\n",
    "relevant_cols = {\n",
    "    \"articles\": ['id', 'title', 'body', 'source_id', 'published_at'],\n",
    "    \"claims\": ['id', 'statement', 'description', 'rating', 'created_at'],\n",
    "    \"fact_checking_articles\": ['id', 'claim', 'description', 'rating', 'source_id', 'published_at'],\n",
    "    \"sources\": ['id', 'name'],\n",
    "    \"entity_annotations\": ['id', 'annotation_type_id', 'entity_type', 'entity_id', 'value'],\n",
    "    \"relation_annotations\": ['id', 'annotation_type_id', 'source_entity_type', 'source_entity_id', 'target_entity_type', 'target_entity_id', 'value']\n",
    "}\n",
    "\n",
    "data: Dict[str, pl.DataFrame] = {}\n",
    "for entity, cols in tqdm(relevant_cols.items()):\n",
    "    data[entity] = pl.read_csv(base.format(entity), columns=cols)\n",
    "\n",
    "lookup: Dict[str, Dict[int, Dict[str, Any]]] = {\n",
    "    name: {row[\"id\"]: row for row in df.to_dicts()} for name, df in tqdm(data.items())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles Columns:\n",
      "id, title, body, source_id, published_at\n",
      "\n",
      "Claims Columns:\n",
      "id, statement, description, rating, created_at\n",
      "\n",
      "Fact Checking Articles Columns:\n",
      "id, claim, description, rating, source_id, published_at\n",
      "\n",
      "Sources Columns:\n",
      "id, name\n",
      "\n",
      "Entity Annotations Columns:\n",
      "id, annotation_type_id, entity_type, entity_id, value\n",
      "\n",
      "Relation Annotations Columns:\n",
      "id, annotation_type_id, source_entity_type, source_entity_id, target_entity_type, target_entity_id, value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Articles Columns:\", ', '.join(data['articles'].columns), sep='\\n', end='\\n\\n')\n",
    "print(\"Claims Columns:\", ', '.join(data['claims'].columns), sep='\\n', end='\\n\\n')\n",
    "print(\"Fact Checking Articles Columns:\", ', '.join(data['fact_checking_articles'].columns), sep='\\n', end='\\n\\n')\n",
    "print(\"Sources Columns:\", ', '.join(data['sources'].columns), sep='\\n', end='\\n\\n')\n",
    "print(\"Entity Annotations Columns:\", ', '.join(data['entity_annotations'].columns), sep='\\n', end='\\n\\n')\n",
    "print(\"Relation Annotations Columns:\", ', '.join(data['relation_annotations'].columns), sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Group 3: 100%|██████████| 416899/416899 [00:11<00:00, 35501.61it/s]\n",
      "Group 4: 100%|██████████| 3292/3292 [00:00<00:00, 30167.20it/s]\n",
      "Group 2: 100%|██████████| 417216/417216 [00:12<00:00, 33097.98it/s]\n",
      "Group 1: 100%|██████████| 70/70 [00:00<00:00, 8223.20it/s]\n",
      "Group 6: 100%|██████████| 110343/110343 [00:02<00:00, 48737.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "\n",
    "for annotation_type_id, group in data['relation_annotations'].group_by('annotation_type_id'):\n",
    "    rs = []\n",
    "    for row in tqdm(group.to_dicts(), desc=f\"Group {annotation_type_id[0]}\"):\n",
    "        source_type = row.pop('source_entity_type')\n",
    "        try:\n",
    "            source = lookup[source_type][row.pop('source_entity_id')].copy()\n",
    "        except KeyError: continue\n",
    "        source = {f'{source_type}_{key}': v for key, v in source.items()}\n",
    "        target_type = row.pop('target_entity_type')\n",
    "        try:\n",
    "            target = lookup[target_type][row.pop('target_entity_id')].copy()\n",
    "        except KeyError: continue\n",
    "        target = {f'{target_type}_{key}': v for key, v in target.items()}\n",
    "        value_dict = row.pop('value').replace('null', 'None')\n",
    "        value = eval(value_dict)\n",
    "        if value: value = value['value']\n",
    "        else: value = None\n",
    "        new_row = {**row, **source, **target, 'value': value}\n",
    "\n",
    "        rs.append(new_row)\n",
    "    dfs[annotation_type_id[0]] = pl.DataFrame(rs)\n",
    "\n",
    "for annotation_type_id, group in data['entity_annotations'].group_by('annotation_type_id'):\n",
    "    rs = []\n",
    "    if annotation_type_id[0] == 6: continue\n",
    "    for row in tqdm(group.to_dicts(), desc=f\"Group {annotation_type_id[0]}\"):\n",
    "        source_type = row.pop('entity_type')\n",
    "        try:\n",
    "            source = lookup[source_type][row.pop('entity_id')].copy()\n",
    "        except KeyError: continue\n",
    "        source = {f'{source_type}_{key}': v for key, v in source.items()}\n",
    "        value_dict = row.pop('value').replace('null', 'None')\n",
    "        value = eval(value_dict)\n",
    "        if value: value = value['value']\n",
    "        else: value = None\n",
    "        new_row = {**row, **source, 'value': value}\n",
    "\n",
    "        rs.append(new_row)\n",
    "    dfs[annotation_type_id[0]] = pl.DataFrame(rs)\n",
    "\n",
    "# 6 is a special case\n",
    "rs = []\n",
    "for row in tqdm(data['entity_annotations'].filter(pl.col('annotation_type_id') == 6).to_dicts(), desc=f\"Group 6\"):\n",
    "    source_type = row.pop('entity_type')\n",
    "    try:\n",
    "        source = lookup[source_type][row.pop('entity_id')].copy()\n",
    "    except KeyError: continue\n",
    "    source = {f'{source_type}_{key}': v for key, v in source.items()}\n",
    "    value_dict = row.pop('value').replace('null', 'None')\n",
    "    value = eval(value_dict)['claims']\n",
    "    for claim in value:\n",
    "        try:\n",
    "            target = lookup['claims'][claim.pop('claim_id')].copy()\n",
    "        except KeyError: continue\n",
    "        target = {f'claims_{key}': v for key, v in target.items()}\n",
    "        new_row = {**row, **source, **target, **claim}\n",
    "        rs.append(new_row)\n",
    "dfs[6] = pl.DataFrame(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3\n",
      "Writing 4\n",
      "Writing 2\n",
      "Writing 1\n",
      "Writing 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "for annotation_type, df in dfs.items():\n",
    "    print(f\"Writing {annotation_type}\")\n",
    "    df.write_csv(f'../data/processed/monant_{annotation_type}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_29540\\2539924737.py:18: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns(df['presence'].map_elements(map_presence).alias('presence'))\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# 1. Merge both for a single-call evaluation\n",
    "df2 = dfs[2].rename({'value':'presence', 'id': '2_id'})\n",
    "df3 = dfs[3].rename({'value':'stance', 'id': '3_id'})\n",
    "\n",
    "df = df2.join(df3, on=[\"articles_id\", \"claims_id\"], how=\"inner\")\n",
    "df = df.select([col for col in df.columns if not col.endswith(\"_right\")])\n",
    "\n",
    "# 2. Unify presence values\n",
    "def map_presence(value):\n",
    "    if value in [\"yes\", \"present\", \"suggestive\"]: return \"yes\"\n",
    "    elif value in [\"no\", \"not-present\"]: return \"no\"\n",
    "    return value\n",
    "\n",
    "df = df.with_columns(df['presence'].map_elements(map_presence).alias('presence'))\n",
    "\n",
    "# 3. Remove not-determined-yet presence and stance\n",
    "df = df.filter(pl.col('presence') != 'not-determined-yet')\n",
    "df = df.filter(pl.col('stance') != 'not-determined-yet')\n",
    "\n",
    "df.write_csv('../data/processed/monant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_29540\\3803221742.py:1: DeprecationWarning: `GroupBy.count` is deprecated. It has been renamed to `len`.\n",
      "  pair_counts = df.group_by([\"articles_id\", \"claims_id\"]).count()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_030, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2_id</th><th>annotation_type_id</th><th>target_entity_type</th><th>articles_id</th><th>articles_title</th><th>articles_body</th><th>articles_source_id</th><th>articles_published_at</th><th>claims_id</th><th>claims_statement</th><th>claims_description</th><th>claims_rating</th><th>claims_created_at</th><th>presence</th><th>3_id</th><th>stance</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>594146</td><td>2</td><td>&quot;claims&quot;</td><td>959847</td><td>&quot;Could Dirty Air Help Speed Alz…</td><td>&quot;By Amy Norton HealthDay Report…</td><td>186</td><td>&quot;2020-12-03 08:00:00+01&quot;</td><td>1872</td><td>&quot;Is air pollution linked to gre…</td><td>&quot;&lt;p&gt;&lt;a href=&quot;https://www.thegua…</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:29:38.661545+01&quot;</td><td>&quot;yes&quot;</td><td>594230</td><td>&quot;supporting&quot;</td></tr><tr><td>594146</td><td>2</td><td>&quot;claims&quot;</td><td>959847</td><td>&quot;Could Dirty Air Help Speed Alz…</td><td>&quot;By Amy Norton HealthDay Report…</td><td>186</td><td>&quot;2020-12-03 08:00:00+01&quot;</td><td>1872</td><td>&quot;Is air pollution linked to gre…</td><td>&quot;&lt;p&gt;&lt;a href=&quot;https://www.thegua…</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:29:38.661545+01&quot;</td><td>&quot;yes&quot;</td><td>902823</td><td>&quot;supporting&quot;</td></tr><tr><td>594129</td><td>2</td><td>&quot;claims&quot;</td><td>959847</td><td>&quot;Could Dirty Air Help Speed Alz…</td><td>&quot;By Amy Norton HealthDay Report…</td><td>186</td><td>&quot;2020-12-03 08:00:00+01&quot;</td><td>1870</td><td>&quot;Does air pollution cause Alzhe…</td><td>&quot;&lt;p&gt;An article published in Sep…</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:29:38.422382+01&quot;</td><td>&quot;yes&quot;</td><td>594213</td><td>&quot;supporting&quot;</td></tr><tr><td>594129</td><td>2</td><td>&quot;claims&quot;</td><td>959847</td><td>&quot;Could Dirty Air Help Speed Alz…</td><td>&quot;By Amy Norton HealthDay Report…</td><td>186</td><td>&quot;2020-12-03 08:00:00+01&quot;</td><td>1870</td><td>&quot;Does air pollution cause Alzhe…</td><td>&quot;&lt;p&gt;An article published in Sep…</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:29:38.422382+01&quot;</td><td>&quot;yes&quot;</td><td>902822</td><td>&quot;supporting&quot;</td></tr><tr><td>595508</td><td>2</td><td>&quot;claims&quot;</td><td>962392</td><td>&quot;Are Scientists Close to a &#x27;Uni…</td><td>&quot;Scientists say they may be get…</td><td>186</td><td>&quot;2020-12-09 08:00:00+01&quot;</td><td>4593</td><td>&quot;Is the Flu Vaccine effective?&quot;</td><td>&quot;&lt;p&gt;I read an &lt;a href=&quot;https://…</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:35:49.362022+01&quot;</td><td>&quot;yes&quot;</td><td>595689</td><td>&quot;neutral&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>571395</td><td>2</td><td>&quot;claims&quot;</td><td>922893</td><td>&quot;FluMist Vs. Flu Shots for Kids…</td><td>&quot;While getting a flu vaccine ea…</td><td>221</td><td>&quot;2020-10-18 11:18:30+02&quot;</td><td>6292</td><td>&quot;The flu shot causes false posi…</td><td>&quot;Claim that flu shot causes fal…</td><td>&quot;false&quot;</td><td>&quot;2020-04-16 20:22:31.611459+02&quot;</td><td>&quot;yes&quot;</td><td>571625</td><td>&quot;contradicting&quot;</td></tr><tr><td>575415</td><td>2</td><td>&quot;claims&quot;</td><td>931121</td><td>&quot;VACCINES KILL: Study finds pos…</td><td>&quot; \n",
       "\n",
       "\n",
       " (Natural News)\n",
       " An Oct. 1…</td><td>145</td><td>&quot;2020-11-03 00:00:00+01&quot;</td><td>6279</td><td>&quot;flu vaccine increases risk of …</td><td>&quot;Claim that flu vaccine increas…</td><td>&quot;false&quot;</td><td>&quot;2020-04-16 20:22:29.90673+02&quot;</td><td>&quot;yes&quot;</td><td>902808</td><td>&quot;supporting&quot;</td></tr><tr><td>575415</td><td>2</td><td>&quot;claims&quot;</td><td>931121</td><td>&quot;VACCINES KILL: Study finds pos…</td><td>&quot; \n",
       "\n",
       "\n",
       " (Natural News)\n",
       " An Oct. 1…</td><td>145</td><td>&quot;2020-11-03 00:00:00+01&quot;</td><td>6279</td><td>&quot;flu vaccine increases risk of …</td><td>&quot;Claim that flu vaccine increas…</td><td>&quot;false&quot;</td><td>&quot;2020-04-16 20:22:29.90673+02&quot;</td><td>&quot;yes&quot;</td><td>577309</td><td>&quot;contradicting&quot;</td></tr><tr><td>586475</td><td>2</td><td>&quot;claims&quot;</td><td>946059</td><td>&quot;Aducanumab isn&amp;#8217;t the sim…</td><td>&quot;This year’s Clinical Trials on…</td><td>165</td><td>&quot;2019-12-20 04:45:59+01&quot;</td><td>449</td><td>&quot;Is alzheimers genetic?&quot;</td><td>&quot;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&quot;</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:26:32.150536+01&quot;</td><td>&quot;yes&quot;</td><td>902755</td><td>&quot;supporting&quot;</td></tr><tr><td>586475</td><td>2</td><td>&quot;claims&quot;</td><td>946059</td><td>&quot;Aducanumab isn&amp;#8217;t the sim…</td><td>&quot;This year’s Clinical Trials on…</td><td>165</td><td>&quot;2019-12-20 04:45:59+01&quot;</td><td>449</td><td>&quot;Is alzheimers genetic?&quot;</td><td>&quot;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&quot;</td><td>&quot;true&quot;</td><td>&quot;2019-12-13 14:26:32.150536+01&quot;</td><td>&quot;yes&quot;</td><td>588097</td><td>&quot;supporting&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_030, 16)\n",
       "┌────────┬─────────────┬────────────┬────────────┬───┬────────────┬──────────┬────────┬────────────┐\n",
       "│ 2_id   ┆ annotation_ ┆ target_ent ┆ articles_i ┆ … ┆ claims_cre ┆ presence ┆ 3_id   ┆ stance     │\n",
       "│ ---    ┆ type_id     ┆ ity_type   ┆ d          ┆   ┆ ated_at    ┆ ---      ┆ ---    ┆ ---        │\n",
       "│ i64    ┆ ---         ┆ ---        ┆ ---        ┆   ┆ ---        ┆ str      ┆ i64    ┆ str        │\n",
       "│        ┆ i64         ┆ str        ┆ i64        ┆   ┆ str        ┆          ┆        ┆            │\n",
       "╞════════╪═════════════╪════════════╪════════════╪═══╪════════════╪══════════╪════════╪════════════╡\n",
       "│ 594146 ┆ 2           ┆ claims     ┆ 959847     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 594230 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:29:38.6 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 61545+01   ┆          ┆        ┆            │\n",
       "│ 594146 ┆ 2           ┆ claims     ┆ 959847     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 902823 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:29:38.6 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 61545+01   ┆          ┆        ┆            │\n",
       "│ 594129 ┆ 2           ┆ claims     ┆ 959847     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 594213 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:29:38.4 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 22382+01   ┆          ┆        ┆            │\n",
       "│ 594129 ┆ 2           ┆ claims     ┆ 959847     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 902822 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:29:38.4 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 22382+01   ┆          ┆        ┆            │\n",
       "│ 595508 ┆ 2           ┆ claims     ┆ 962392     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 595689 ┆ neutral    │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:35:49.3 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 62022+01   ┆          ┆        ┆            │\n",
       "│ …      ┆ …           ┆ …          ┆ …          ┆ … ┆ …          ┆ …        ┆ …      ┆ …          │\n",
       "│ 571395 ┆ 2           ┆ claims     ┆ 922893     ┆ … ┆ 2020-04-16 ┆ yes      ┆ 571625 ┆ contradict │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 20:22:31.6 ┆          ┆        ┆ ing        │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 11459+02   ┆          ┆        ┆            │\n",
       "│ 575415 ┆ 2           ┆ claims     ┆ 931121     ┆ … ┆ 2020-04-16 ┆ yes      ┆ 902808 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 20:22:29.9 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 0673+02    ┆          ┆        ┆            │\n",
       "│ 575415 ┆ 2           ┆ claims     ┆ 931121     ┆ … ┆ 2020-04-16 ┆ yes      ┆ 577309 ┆ contradict │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 20:22:29.9 ┆          ┆        ┆ ing        │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 0673+02    ┆          ┆        ┆            │\n",
       "│ 586475 ┆ 2           ┆ claims     ┆ 946059     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 902755 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:26:32.1 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 50536+01   ┆          ┆        ┆            │\n",
       "│ 586475 ┆ 2           ┆ claims     ┆ 946059     ┆ … ┆ 2019-12-13 ┆ yes      ┆ 588097 ┆ supporting │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 14:26:32.1 ┆          ┆        ┆            │\n",
       "│        ┆             ┆            ┆            ┆   ┆ 50536+01   ┆          ┆        ┆            │\n",
       "└────────┴─────────────┴────────────┴────────────┴───┴────────────┴──────────┴────────┴────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all rows where 'articles_id' and 'claims_id' are duplicated\n",
    "pair_counts = df.group_by([\"articles_id\", \"claims_id\"]).count()\n",
    "valid_pairs = pair_counts.filter(pair_counts[\"count\"] > 1).select([\"articles_id\", \"claims_id\"])\n",
    "df.join(valid_pairs, on=[\"articles_id\", \"claims_id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
